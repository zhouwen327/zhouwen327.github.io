<!DOCTYPE html>
<html>
  <head>
    <title>äº¤äº’å¼è™šæ‹Ÿç°å®ä¸æœºå™¨å­¦ä¹ å®éªŒå®¤</title>
    <link href="./style.css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <html>
      <head>
        <title>äº¤äº’å¼è™šæ‹Ÿç°å®ä¸æœºå™¨å­¦ä¹ å®éªŒå®¤ Interactive Virtual reality and Machine leanring Lab</title>
        <meta charset="UTF-8">
        <meta content="width=device-width, initial-scale=1" name="viewport">
      </head>
      <body></body>
    </html>
    <nav id="nav_head">
      <a href="#a0">æœŸåˆŠè®ºæ–‡</a>
      <a href="#a1">ä¼šè®®è®ºæ–‡</a>
      <a href="#a2">ç§‘ç ”é¡¹ç›®</a>
      <a href="#a3">æ•™ç ”é¡¹ç›®</a>
      <a href="#a4">ä¸“åˆ©ä¸è½¯è‘—</a>
      <a href="#a5">å­¦æœ¯å®¡ç¨¿</a>
      <a href="#a6">æ•°æ®é›†</a>
      <a href="#a7">å…¶ä»–</a>
      <a href="https://zhouwen327.github.io/" target="_blank">English</a>
    </nav>
    <section id="a0">
      <a href="#nav_head" id="title_">ğŸš€æœŸåˆŠè®ºæ–‡â†‘</a>
      <ol>
        <li id="j_li">Wangyu Shen, <u><b>Wen Zhou</b></u>, <span id="j_name">A novel Internet of medical things framework for absorbing bioresorbable vascular scaffold towards healthcare monitoring based on improving YOLO paradigms[J]</span>//<i>Knowledge-based Systems</i>,2025,113696. (CCF-BæœŸåˆŠ)[<a href=http://ivr-ahnu.cn/cn/paper/2025a2.pdf  target=_blank>PDF</a>][<a href=https://doi.org/10.1016/j.knosys.2025.113696 target=_blank>DOI</a>]
</li>
        <li id="j_li"><u><b>Wen Zhou</b></u>,Wangyu Shen, Xinyi Meng, <span id="j_name">An improved social force model-driven multiagent generative adversarial imitation learning framework for pedestrian trajectory prediction.[J]</span> <i>Computer Animation and Virtual Worlds</i> 2025, 36(1): e2035.(CCF-CæœŸåˆŠ)[<a href=http://ivr-ahnu.cn/cn/paper/2025a1.pdf  target=_blank>PDF</a>][<a href=https://doi.org/10.1002/cav.70058 target=_blank>DOI</a>]
</li>
        <li id="j_li">å¼ æ™¨,è’‹æ–‡è‹±,é™ˆæ€æº, <u><b>å‘¨æ–‡</b></u>, é—«ä¸°äº­, <span id="j_name">åŸºäºåŒå±‚DQNçš„å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’ç®—æ³•[J]</span>//<i>ä¸­å›½å›¾è±¡å›¾å½¢å­¦æŠ¥</i>, 2023,28(7):2167-2181. (é€šè®¯ä½œè€…)[<a href=http://ivr-ahnu.cn/cn/paper/2022b1.pdf target="_blank">PDF][<a href='https://doi.org/10.11834/jig.211239' target='_black'>DOI</a>]
</li>
        <li id="j_li">Siyuan Chen, Xiaowu Hu, Wenying Jiang, <u><b>Wen Zhou</b></u>, Xintao Ding, <span id="j_name">Novel learning framework for optimal multi-object video trajectory tracking[J]</span>//<i>Virtual Reality & Intelligent Hardware</i>,2023,5(5):422-438.[<a href=http://ivr-ahnu.cn/cn/paper/2023b.pdf  target=_blank>PDF</a>][<a href=https://doi.org/10.1016/j.vrih.2023.04.001 target=_blank>DOI</a>][cited:1]
</li>
        <li id="j_li"><u><b>W. Zhou</b></u>, C. Zhang, S. Chen, <span id="j_name">Dual deep Q-learning network guiding a multiagent path planning approach for virtual fire emergency scenarios[J]</span>//<i>Applied Intelligence</i>, 2023, 53:21858â€“21874.[<a href=http://ivr-ahnu.cn/cn/paper/2023a1.pdf  target=_blank>PDF</a>][<a href=https://doi.org/10.1007/s10489-023-04601-9 target=_blank>DOI</a>][cited:11]
</li>
        <li id="j_li">Huang C., Xu G., Chen S., <u><b>Zhou Wen</b></u>, NG E. Y., & de Albuquerque, Victor Hugo C. de Albuquerque, <span id="j_name">An Improved Federated Learning Approach Enhanced Internet of Health Things Framework for Private Decentralized Distributed Data[J]</span>//<i>Information Sciences</i>, 2022, 614: 138-152. (CCF-BæœŸåˆŠï¼Œé€šè®¯ä½œè€…)[<a href=http://ivr-ahnu.cn/cn/paper/2022b2.pdf  target=_blank>PDF</a>][<a href=https://doi.org/10.1007/s10489-023-04601-9 target=_blank>DOI</a>]
</li>
        <li id="j_li">C. Huang, S. Chen, Y. Zhang, <u><b>W. Zhou</b></u>, J. Rodrigues and V. H. C. de Albuquerque, <span id="j_name">A robust approach for privacy data protection: IoT security assurance using generative adversarial imitation learning[J]</span>//<i>IEEE Internet of Things Journal</i>,2022,9(18):17089 â€“ 17097. (é€šè®¯ä½œè€…)[<a href=http://ivr-ahnu.cn/cn/paper/2022a1.pdf  target=_blank>PDF</a>][<a href=https://doi.org/10.1016/j.ins.2022.10.011 target=_blank>DOI</a>][cited:28][IF:6.8/Q1]
</li>
        <li id="j_li"><u><b>W. Zhou</b></u>, W. Jiang, B. Jie, W. Bian, <span id="j_name">Multiagent Evacuation Framework for a Virtual Fire Emergency Scenario Based on Generative Adversary Imitation Learning[J]</span>//<i>Computer Animation and Virtual Worlds</i>, 2022,33(1):e2035. (CCF-CæœŸåˆŠ)[<a href="" target=_blank>PDF</a>][<a href="https://doi.org/10.1002/cav.2035" target=_blank>DOI</a>][cited:7][IF:1.7/Q3]
</li>
        <li id="j_li"><u><b>W. Zhou</b></u>, J. Jia, W. Jiang, C. Huang, <span id="j_name">Sketch Augmentation-driven Shape Retrieval Learning Framework Based on Convolutional Neural Networks[J]</span>//<i>IEEE Transactions on Visualization and Computer Graphics</i>, 2021,27(8):3558 â€“ 3570. (CCF-AæœŸåˆŠ)[<a href=http://ivr-ahnu.cn/cn/paper/2021a1.pdf  target=_blank>PDF</a>][<a href=https://doi.org/10.1109/TVCG.2020.2975504 target=_blank>DOI</a>][cited:8]
</li>
        <li id="j_li"><u><b>W. Zhou</b></u>, J. Jia,<span id="j_name"> Training convolutional Neural Network for sketch recognition on large-scale datasets[J]</span>//<i>The International Arab Journal of Information Technology</i>,2020,17(1):82-89. (SCI).[<a href=http://ivr-ahnu.cn/cn/paper/2020a3.pdf  target=_blank>PDF</a>][<a href="" target=_blank>DOI</a>][cited:9]
</li>
        <li id="j_li"><u><b>W. Zhou</b></u>, J. Jia,<span id="j_name">Training Deep Convolutional Neural Networks to Acquire the Best View of a 3D Shape[J]</span>//<i>Multimedia Tools and Applications</i>. 2020, 79(1):581-60 (CCF-CæœŸåˆŠ)[<a href=http://ivr-ahnu.cn/cn/paper/2020a2.pdf  target=_blank>PDF</a>][<a href="https://doi.org/10.1007/s11042-019-08107-w" target=_blank>DOI</a>][cited:4]
</li>
        <li id="j_li"><u><b>W. Zhou</b></u>, J. Jia, C. Huang, Y. CHENG,<span id="j_name">Web3D learning framework for 3D Shape retrieval based on hybrid Convolutional Neural Networks[J]</span>//<i>Tsinghua Science and Technology</i>,2020, 25(1): 93-102.[<a href=http://ivr-ahnu.cn/cn/paper/2020a1.pdf  target=_blank>PDF</a>][<a href="https://doi.org/10.26599/TST.2018.9010113" target=_blank>DOI</a>][cited:22]
</li>
        <li id="j_li"><u><b>W. Zhou</b></u>, J. Jia, <span id="j_name">A Learning Framework for Shape Retrieval Based on Multilayer Perceptrons[J]</span><i>Pattern Recognition Letters</i>, 2019,1(117):119-130.(CCF-CæœŸåˆŠ)[<a href=http://ivr-ahnu.cn/cn/paper/2019a1.pdf  target=_blank>PDF</a>][<a href="https://doi.org/10.1016/j.patrec.2018.09.005" target=_blank>DOI</a>][cited:22][IF:3.3/Q2]
</li>
        <li id="j_li"><u><b>å‘¨æ–‡</b></u>,è´¾é‡‘åŸ,<span id="j_name">ä¸€ç§SVMå­¦ä¹ æ¡†æ¶ä¸‹çš„Web3Dè½»é‡çº§æ¨¡å‹æ£€ç´¢ç®—æ³•[J]</span>//<i>ç”µå­å­¦æŠ¥</i>,2019,47(1):92-99.[<a href=http://ivr-ahnu.cn/cn/paper/2019a2.pdf  target=_blank>PDF</a>][<a href="" target=_blank>DOI</a>][cited:4]
</li>
        <li id="j_li"><u><b>W. Zhou</b></u>, K. TANG, J. Jia, <span id="j_name">S-LPM: Segmentation Augmented Light-weighting and Progressive Meshing for the Interactive Visualization of Large Man-made Web3D Models[J]</span>//<i>World Wide Web Journal</i>, 2018,21(5):1425â€“1448 (CCF-BæœŸåˆŠ)[<a href=http://ivr-ahnu.cn/cn/paper/2018b.pdf  target=_blank>PDF</a>][<a href=https://doi.org/10.1007/s11280-018-0610-1 target=_blank>DOI</a>][cited:11][IF:3.4/Q2]
</li>
        <li id="j_li"><u><b>Zhou W</b></u>, Jia J, Su X,<span id="j_name"> A novel compression-driven lightweight framework for medical skeleton model visualization</span>[J]// <i>IEEE Access</i>, 2018, 6: 47627-47635.
</li>
        <li id="j_li">Huang C, <u><b>Zhou W*</b></u>, Lan Y, et al,<span id="j_name"> A novel WebVR-based lightweight framework for virtual visualization of blood vasculum</span>[J]//<i> IEEE Access</i>, 2018, 6: 27726-27735.
</li>
        <li id="j_li">Huang C, Lan Y, Liu Y, <b><u>Zhou Wen</u></b>,et al,<span id="j_name"> A new dynamic path planning approach for unmanned aerial vehicles</span>[J]//<i> Complexity</i>, 2018, 2018(1): 8420294.
</li>
        <li id="j_li"><b><u>å‘¨æ–‡</u></b>, è´¾é‡‘åŸ. <span idj_name>å¯å‘å¼è½®å»“çº¿ä¸å˜çš„ç½‘æ ¼è‡ªé€‚åº”ç®€åŒ–ç®—æ³•</span>[J]// <i>ç³»ç»Ÿä»¿çœŸå­¦æŠ¥</i>, 2016, 28(9): 2176-2185.[<a HREF=https://www.china-simulation.com/CN/Y2016/V28/I9/2176 target="_blank">WEB</a>]
</li>
        <li id="j_li"><b><u>å‘¨æ–‡</u></b>,è´¾é‡‘åŸ,æ¸©æ¥ç¥¥,ç­‰ï¼Œ<span id=j_name>è½»é‡çº§ä¸‰ç»´æ¨¡å‹ç´ æåº“åŠå…¶å…³é”®æŠ€æœ¯çš„ç ”ç©¶</span>[J]//<i>ç³»ç»Ÿä»¿çœŸå­¦æŠ¥</i>, 2015, 27(10): 2514-2524.[<A HREF=https://www.china-simulation.com/CN/Y2015/V27/I10/2514 target="_blank">WEB</A>]
</li>
        <li id="j_li">èµµåŒç‡•,è´¾é‡‘åŸ,<b><u>å‘¨æ–‡</u></b>,<span id=j_name>Web3Då®¶å±…ç´ æåº“çš„è½»é‡åŒ–æŠ€æœ¯ç ”ç©¶</span>[J]//<i>éƒ‘å·å¤§å­¦å­¦æŠ¥(å·¥å­¦ç‰ˆ)</i>,2019,40(01):12-17.[<A HREF=http://gxb.zzu.edu.cn/oa/DArticle.aspx?type=view&id=201803017 target="_blank">WEB</A>]</li>
      </ol>
    </section>
    <section id="a1">
      <a href="#nav_head" id="title_">ğŸš€ä¼šè®®è®ºæ–‡â†‘</a>
      <ol>
        <li id="c_li">Wangyu Shen, <b><u>Wen Zhou</u></b><span id="j_name">Focus-YOLO: An improved multiobject analysis framework for crop monitoring and pest prevention</span>[C]//<i>International Conference of Pioneering Computer Scientists, Engineers and Educators (ICPCSEE 2025)</i>,September,2025,Hiroshima University,Japan.
</li>
        <li id="c_li">Chuanyu Yao, <b><u>Wen Zhou</u></b>,<span id="j_name">Attention Graph Neural Controlled Differential Equations approach for Traffic Flow Forecasting</span>[C]//<i>The 6th International Conference on Pattern recognition and Machine learning (PRML 2025)</i>June 2025,Chongqing,China.
</li>
        <li id="c_li">Wangyu Shen, Xinyi Meng, <b><u>Wen Zhou</u></b>,<span id="j_name">Multiagent Virtual Community Drill Approach Based on A3C Paradigm</span>[C]//<i>The 6th International Conference on Computer Science,Engineering, and Education (CSEE 2025)</i>,Feburary,2025,Nanjing,China.
</li>
        <li id="c_li"><b><u>Wen ZHOU</u></b>, Jinyuan JIA,<span id="j_name">S-LCM: Compression-driven Web3D Lightweight framework for Mesh Visualization</span> [C]//<i> Proceedings - 2017 International Conference on Virtual Reality and Visualization (ICVRV 2017) </i>(EI, 20192407048176)
</li>
        <li id="c_li"><b><u>Wen ZHOU</u></b>, Jinyuan JIA and Shuang Liang,<span id="j_name">View-dependent simplification for Web3D triangular mesh based on voxelization and saliency</span>[C]//<i>Proceedings - 2016 International Conference on Virtual Reality and Visualization(ICVRV 2016)</i>, 2017: 280â€“285.(EIï¼Œ20173003980152)
</li>
        <li id="c_li"><b><u>Wen ZHOU</u></b> , Jinyuan JIA, <span id="j_name">Lightweight Web3D Visualization Framework Using Dijkstra-Based Mesh Segmentation</span>[C]//<i>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</i>, 2017:138â€“151. (EIï¼Œ20174404357376)
</li>
      </ol>
    </section>
    <section id="a2">
      <a href="#nav_head" id="title_">ğŸš€ç§‘ç ”é¡¹ç›®â†‘</a>
      <ol>
        <li id="my_li">å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘é’å¹´é¡¹ç›® <b><u>(No.61902003)</u></b> ,<span id=p_name>é¢å‘WebVRçš„è½»é‡çº§ä¸‰ç»´æ¨¡å‹æ£€ç´¢å…³é”®æŠ€æœ¯ç ”ç©¶</span>,2020.1-2022.12,<b>ä¸»æŒ</b> (<span style='color:green;font-weight:bolder'>å·²ç»“é¢˜</span>)
</li>
        <li id="my_li">å®‰å¾½çœé«˜æ ¡ç§‘ç ”é‡ç‚¹é¡¹ç›®<b><u>(No.2023AH050142)</u></b>,<span id=p_name>é¢å‘è™šæ‹Ÿåœºæ™¯çš„è™šå®èåˆå­¦ä¹ å’Œäº¤äº’æ–¹æ³•çš„ç ”ç©¶</span>,2023.9-2025.12,<b>ä¸»æŒ</b> (<span style='color:blue;font-weight:bolder'>åœ¨ç ”</span>)
</li>
        <li id="my_li">å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘é¢ä¸Šé¡¹ç›®<b><u>(No.61976006)</u></b>,<span id=p_name>é¢å‘åŠŸèƒ½ç£å…±æŒ¯æˆåƒçš„åŠ¨æ€è„‘ç½‘ç»œåˆ†æåŠåº”ç”¨ç ”ç©¶</span>,2020.1-2023.12,<u>å‚ä¸</u> (<span style='color:green;font-weight:bolder'>å·²ç»“é¢˜</span>)
</li>
      </ol>
    </section>
    <section id="a3">
      <a href="#nav_head" id="title_">ğŸš€æ•™ç ”é¡¹ç›®â†‘</a>
      <ol>
        <li id="my_li">å®‰å¾½çœè´¨é‡å·¥ç¨‹ä¸€èˆ¬é¡¹ç›®<b><u>(No. 2022jyxm563)</u></b>,æ•°å­—å›¾åƒå¤„ç†è¯¾ç¨‹å¤šå…ƒæ··åˆå¼æ•™å­¦æ–¹æ³•çš„æ¢ç´¢ä¸å®è·µ,2023-2025.(<b>ä¸»æŒ</b>)
</li>
        <li id="my_li">å®‰å¾½å¸ˆèŒƒå¤§å­¦æ•™ç ”é¡¹ç›®<b><u>(æ ¡æ•™å­—ã€”2020ã€•44 å·)</u></b>,'æ–°å·¥ç§‘'å»ºè®¾èƒŒæ™¯ä¸‹äººå·¥æ™ºèƒ½ä¸“ä¸šæ•™å­¦æ¨¡å¼çš„ç ”ç©¶,2022-2024.(<b>ä¸»æŒ</b>)
</li>
        <li id="my_li">å®‰å¾½çœè´¨é‡å·¥ç¨‹è™šæ‹Ÿä»¿çœŸå®éªŒæ•™å­¦è¯¾ç¨‹<b><u>(No.2020xfxm23)</u></b>, è®¡ç®—æœºç»„æˆè™šæ‹Ÿå®éªŒï¼Œ2022-2024.(<u>å‚ä¸</u>)
</li>
        <li id="my_li">å®‰å¾½çœè´¨é‡å·¥ç¨‹æ•™å­¦å›¢é˜Ÿ<b><u>(çš–æ•™ç§˜é«˜[2020]155å·)</u></b>,è®¡ç®—æœºç³»ç»Ÿèƒ½åŠ›åŸ¹å…»æ•™å­¦å›¢é˜Ÿ, 2020-2022.(<u>å‚ä¸</u>)
</li>
        <li id="my_li">å®‰å¾½å¸ˆèŒƒå¤§å­¦æ•™ç ”é¡¹ç›®æ•™å­¦å›¢é˜Ÿ<b><u>(æ ¡æ•™å­—ã€”2020ã€•44 å·)</u></b>,ç¨‹åºè®¾è®¡åŸºç¡€, 2020-2022.(<u>å‚ä¸</u>)</li>
      </ol>
    </section>
    <section id="a4">
      <a href="#nav_head" id="title_">ğŸš€ä¸“åˆ©ä¸è½¯è‘—â†‘</a>
      <ol>
        <li id="p_li"><u>è™šæ‹Ÿç«ç¾é€ƒç”Ÿæ¼”ç»ƒç³»ç»ŸV1.0.</u> (<b>è½¯è‘—ç™»è®°å·:2022SR0438966</b>)[<a href='http://ivr-ahnu.cn/sr/202201.pdf' target="_blank">PDF</a>]
</li>
        <li id="p_li"><u>è™šæ‹Ÿæ•™å®¤è‡ªåŠ¨å¸ƒå±€è®¾è®¡ç³»ç»Ÿv1.0. </u>(<b>è½¯è‘—ç™»è®°å·:2022SR0973155</b>)[<a HREF="http://ivr-ahnu.cn/sr/202202.pdf" target="_blank">PDF</a>]
</li>
        <li id="p_li"><u>åŸºäºç¤¾äº¤åª’ä½“æ–‡æœ¬åˆ†æçš„åŒºåŸŸäº¤é€šäº‹ä»¶å®æ—¶æ£€æµ‹ä¸é¢„è­¦ç³»ç»Ÿ</u>(<b>è½¯è‘—ç™»è®°å·:2025SR0563810</b>)[<a href=http://ivr-ahnu.cn/sr/202504.pdf target="_blank">PDF</a>]</li>
      </ol>
    </section>
    <section id="a5">
      <a href="#nav_head" id="title_">ğŸš€å­¦æœ¯å®¡ç¨¿â†‘</a>
      <ol>
        <li id="my_li">2018-now: IEEE Access
</li>
        <li id="my_li">2018-now: Journal of Grid computing
</li>
        <li id="my_li">2020-now: Virtual Reality & Intelligent Hardware
</li>
        <li id="my_li">2021-2025: PC member of the International symposium on Emerging Technologies for Education (SETE)
</li>
        <li id="my_li">2022-2025: PC member of the ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry (VRCAI)
</li>
        <li id="my_li">2022-2025: PC member of The International Conference on Virtual Reality (ICVR)
</li>
        <li id="my_li">2024-now: Knowledge-based Systems
</li>
        <li id="my_li">2025:Engineering science and technology, an international journal
</li>
        <li id="my_li">2025:Process safety and environmental protection. 
</li>
        <li id="my_li">2025:ç¨‹åºå§”å‘˜ä¼šå§”å‘˜ç¬¬åå±Šä¸­å›½å›¾å­¦å¤§ä¼šï¼ˆChina Graphics'2025ï¼‰</li>
      </ol>
    </section>
    <section id="a6">
      <a href="#nav_head" id="title_">ğŸš€æ•°æ®é›†â†‘</a>
      <ol>
        <li id="my_li"><a href="javascript:;" class="dataset_title">åŒ»å­¦é…å‡†æ•°æ®é›†</a> <div id="_detail"><div><ol class="first_ol"><li>OASIS: brainæ•°æ®é›†,T1w MRI, Alzheimerâ€™s diseaseã€<a href="https://sites.wustl.edu/oasisbrains/" target='_blank'>ä¸‹è½½</a>ã€‘</li><li>DIR-Lab: Lungæ•°æ®é›†,Breath-hold and 4DCT, Alzheimerâ€™s diseaseã€<a href="https://med.emory.edu/departments/radiation-oncology/research-laboratories/deformable-image-registration/index.html" target='_blank'>ä¸‹è½½</a>ã€‘</li><li>IXI: brainæ•°æ®é›†,T1w, T2w, PDw MRI, ã€<a href="https://brain-development.org/ixi-dataset/" target='_blank'>ä¸‹è½½</a>ã€‘</li><li>Lung-CT: è‚ºéƒ¨æ•°æ®é›†,Inspiratory, expiratory CT, ã€<a href="https://learn2reg.grand-challenge.org/" target='_blank'>ä¸‹è½½</a>ã€‘</li><li>ACROBAT: Breastæ•°æ®é›†,Pathological images, ã€<a href="https://acrobat.grand-challenge.org/" target='_blank'>ä¸‹è½½</a>ã€‘</li><li>Abdomen-MR-CT: è…¹éƒ¨æ•°æ®é›†,CT|MR, ã€<a href="https://learn2reg.grand-challenge.org/" target='_blank'>ä¸‹è½½</a>ã€‘</li><li>MM-WHS: å¿ƒè„æ•°æ®é›†,CT|MRI, ã€<a href="https://zmiclab.github.io/zxh/0/mmwhs/" target='_blank'>ä¸‹è½½</a>ã€‘</li><li>ACDC: å¿ƒè„æ•°æ®é›†,4D cine-MRI, ã€<a href="https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html" target='_blank'>ä¸‹è½½</a>ã€‘</li><li><a href='http://ivr-ahnu.cn/cn/ImageRegister.html' target="_blank">æ›´å¤š...</a></li></ol></div></div>
</li>
        <li id="my_li"><a href="https://pan.baidu.com/s/1bjdfFAU8gHZBTWjWNI8ErA?pwd=1234" target="_blank" class="dataset_title">è¡€ç®¡æ”¯æ¶Stent OCT </a><ol><li>Train: 276 OCT images and mask json files, total 342MB</li><li>Test: 42 OCT images, total 121MB</li></ul></div></div></li>
      </ol>
    </section>
    <section id="a7">
      <a href="#nav_head" id="title_">ğŸš€å…¶ä»–â†‘</a>
      <ol>
        <li id="my_li">æš‚æ— <div id="_detail">æš‚æ— </div></li>
      </ol>
    </section>
    <div id="tail">
      <div style="align:center;font-size:12px;color:#aaa;margin-top:10px;">Copyright Â© IVR Lab
</div>
      <div style="color:#aaa;font-size:12px;margin:10px 0px;">Latest updated: 2025-08-30 18:07</div>
    </div>
  </body>
</html>